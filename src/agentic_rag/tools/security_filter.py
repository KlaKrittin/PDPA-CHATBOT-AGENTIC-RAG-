import os
import re
from typing import Dict, List, Tuple, Optional
import logging

# Optional: Guardrails profanity validator
try:
    from guardrails.validators import ProfanityFree  # type: ignore
except Exception:  # pragma: no cover
    ProfanityFree = None  # type: ignore

# Optional: OpenAI-compatible client for llama.cpp server
try:
    from openai import OpenAI  # type: ignore
except Exception:  # pragma: no cover
    OpenAI = None  # type: ignore

class SecurityFilter:
    """
    Security filter for PDPA Assistant to prevent inappropriate content and restrict topics to PDPA only.
    """
    
    def __init__(self):
        # --- External integrations configuration ---
        self.llama_base_url = os.getenv("LLAMA_CPP_BASE_URL", "http://localhost:8080/v1")
        self.llama_model = os.getenv("LLAMA_CPP_MODEL", os.getenv("OLLAMA_MODEL", "hf.co/scb10x/typhoon2.1-gemma3-4b-gguf:Q4_K_M"))
        self._openai_client = None
        if OpenAI is not None:
            try:
                # llama.cpp server ignores api_key by default; set stub
                self._openai_client = OpenAI(base_url=self.llama_base_url, api_key=os.getenv("OPENAI_API_KEY", "not-needed"))
            except Exception:
                self._openai_client = None

        # Guardrails profanity validator instance (optional)
        self._profanity_validator = None
        if ProfanityFree is not None:
            try:
                self._profanity_validator = ProfanityFree()
            except Exception:
                self._profanity_validator = None
        # Realguide-style inappropriate patterns - only truly inappropriate content
        # 1) Define Thai inappropriate terms as a list for maintainability
        self.thai_inappropriate_terms = [
            "‡∏´‡∏µ", "‡∏à‡∏¥‡πã‡∏°", "‡∏à‡∏π‡πã", "‡πÑ‡∏Ç‡πà", "‡∏´‡∏≥", "‡πÅ‡∏ï‡∏î", "‡∏´‡∏±‡∏ß‡∏ô‡∏°", "‡πÄ‡∏¢‡πá‡∏î",
            "‡∏õ‡∏µ‡πâ", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ö", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ß", "‡πÄ‡∏á‡∏µ‡πà‡∏¢‡∏ô", "‡∏Ç‡πà‡∏°‡∏Ç‡∏∑‡∏ô", "‡∏£‡∏∏‡∏°‡πÇ‡∏ó‡∏£‡∏°",
            "‡∏ô‡πâ‡∏≥‡πÅ‡∏ï‡∏Å", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏ô", "‡∏™‡∏ß‡∏¥‡∏á‡∏Å‡∏¥‡πâ‡∏á", "‡∏™‡∏ß‡∏¥‡πâ‡∏á‡∏Å‡∏¥‡πâ‡∏á", "‡∏î‡∏π‡∏î‡∏õ‡∏≤‡∏Å", "‡∏≠‡∏°‡∏Ñ‡∏ß‡∏¢", "‡πÄ‡∏•‡∏µ‡∏¢‡∏´‡∏µ",
            "‡∏ï‡∏π‡∏î", "‡∏™‡πâ‡∏ô‡∏ï‡∏µ‡∏ô", "‡∏ï‡∏µ‡∏ô", "‡∏ï‡∏≠‡πÅ‡∏´‡∏•", "‡∏û‡πà‡∏≠‡∏á", "‡∏û‡πà‡∏≠‡∏°‡∏∂‡∏á", "‡πÅ‡∏°‡πà‡∏°‡∏∂‡∏á", "‡∏û‡πà‡∏≠‡∏°‡∏∂‡∏á‡∏ï‡∏≤‡∏¢",
            "‡πÅ‡∏°‡πà‡∏°‡∏∂‡∏á‡∏ï‡∏≤‡∏¢", "‡πÄ‡∏´‡∏µ‡πâ‡∏¢", "‡πÄ‡∏Æ‡∏µ‡πâ‡∏¢", "‡πÄ‡∏´‡πâ", "‡∏´‡πà‡∏≤", "‡∏™‡∏±‡∏î", "‡∏™‡∏±‡∏™", "‡πÄ‡∏ä‡∏µ‡πà‡∏¢",
            "‡πÄ‡∏ä‡∏µ‡πâ‡∏¢", "‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡πÄ‡∏≠‡πâ‡∏¢", "‡πÄ‡∏ä‡∏µ‡πâ‡∏¢‡πÄ‡∏≠‡πâ‡∏¢", "‡πÅ‡∏°‡πà‡∏á", "‡∏°‡∏∂‡∏á", "‡∏Å‡∏π", "‡πÑ‡∏≠‡∏™‡∏±‡∏î", "‡πÑ‡∏≠‡πâ‡∏™‡∏±‡∏î",
            "‡πÑ‡∏≠‡∏™‡∏±‡∏™", "‡πÑ‡∏≠‡πâ‡∏™‡∏±‡∏™", "‡πÑ‡∏≠‡πâ‡πÄ‡∏´‡∏µ‡πâ‡∏¢", "‡∏≠‡∏µ‡πÄ‡∏´‡∏µ‡πâ‡∏¢", "‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏°‡∏≤‡∏Å", "‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏™‡∏∏‡∏î", "‡πÑ‡∏≠‡πâ‡∏´‡πà‡∏≤",
            "‡πÑ‡∏≠‡∏™‡∏≤‡∏î", "‡πÑ‡∏≠‡∏™‡∏≤‡∏î‡∏î", "‡∏™‡∏±‡∏ï‡∏ß‡πå", "‡∏ä‡∏≤‡∏ï‡∏¥‡∏´‡∏°‡∏≤", "‡πÑ‡∏≠‡πâ‡∏ä‡∏≤‡∏ï‡∏¥‡∏´‡∏°‡∏≤", "‡πÑ‡∏≠‡πâ‡πÄ‡∏ß‡∏£", "‡πÑ‡∏≠‡πâ‡πÄ‡∏ß‡∏£‡∏ï‡∏∞‡πÑ‡∏•",
            "‡πÑ‡∏≠‡πâ‡∏Ñ‡∏ß‡∏≤‡∏¢", "‡∏Ñ‡∏ß‡∏≤‡∏¢", "‡πÑ‡∏≠‡πâ‡πÇ‡∏á‡πà", "‡πÇ‡∏á‡πà‡πÄ‡∏á‡πà‡∏≤", "‡πÇ‡∏á‡πà‡∏Ñ‡∏ß‡∏≤‡∏¢", "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏≠‡πà‡∏≠‡∏ô", "‡πÑ‡∏£‡πâ‡∏™‡∏°‡∏≠‡∏á",
            "‡∏™‡∏°‡∏≠‡∏á‡∏´‡∏°‡∏≤", "‡πÑ‡∏≠‡πâ‡∏ö‡πâ‡∏≤", "‡∏™‡∏ß‡∏∞", "‡πÄ‡∏Æ‡∏á‡∏ã‡∏ß‡∏¢", "‡∏£‡∏∞‡∏¢‡∏≥", "‡πÑ‡∏≠‡πâ‡∏£‡∏∞‡∏¢‡∏≥", "‡∏™‡∏ñ‡∏∏‡∏ô", "‡πÑ‡∏≠‡πâ‡∏™‡∏ñ‡∏∏‡∏ô",
            "‡∏ï‡πà‡∏≥‡∏ï‡∏°", "‡∏≠‡∏±‡∏õ‡∏£‡∏µ‡∏¢‡πå", "‡∏à‡∏±‡∏ç‡πÑ‡∏£", "‡∏Å‡∏£‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏Å‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏Å‡∏£‡∏∞‡∏£‡∏µ‡πà", "‡∏Å‡∏∞‡∏£‡∏µ‡πà", "‡∏≠‡∏µ‡∏ï‡∏±‡∏ß", "‡∏≠‡∏µ‡πÅ‡∏û‡∏®‡∏¢‡∏≤", "‡πÅ‡∏û‡∏®‡∏¢‡∏≤",
            "‡∏≠‡∏µ‡∏î‡∏≠‡∏Å", "‡∏î‡∏≠‡∏Å‡∏ó‡∏≠‡∏á", "‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ",
            "‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢", "‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ß‡∏≠‡∏î", "‡πÑ‡∏≠‡πâ‡∏™‡∏≤‡∏£‡πÄ‡∏•‡∏ß", "‡∏™‡∏≤‡∏£‡πÄ‡∏•‡∏ß", "‡πÄ‡∏•‡∏ß", "‡∏™‡∏ñ‡∏∏‡∏ô‡∏°‡∏≤‡∏Å",
            "‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏≤‡∏ô", "‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏ô‡∏≤", "‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏°‡∏µ‡∏¢", "‡∏Å‡∏≤‡∏Å", "‡∏Ç‡∏¢‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°",
            "‡∏Ñ‡∏ß‡∏¢‡πÉ‡∏´‡∏ç‡πà", "‡∏Ñ‡∏ß‡∏¢‡∏¢‡∏≤‡∏ß", "‡∏Ñ‡∏ß‡∏¢‡πÄ‡∏î‡πá‡∏Å", "‡∏´‡∏µ‡∏ö‡∏≤‡∏ô", "‡∏´‡∏µ‡πÄ‡∏î‡πá‡∏Å", "‡∏´‡∏µ‡πÄ‡∏ô‡πà‡∏≤", "‡∏à‡∏¥‡πã‡∏°‡πÄ‡∏î‡πá‡∏Å",
            "‡πÅ‡∏ï‡∏Å‡∏Ñ‡∏≤‡∏õ‡∏≤‡∏Å", "‡πÅ‡∏ï‡∏Å‡∏Ñ‡∏≤", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏™‡πà‡∏´‡∏ô‡πâ‡∏≤", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏™‡πà‡∏õ‡∏≤‡∏Å", "‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏ô", "‡∏Ç‡∏¢‡πà‡∏°", "‡∏Ç‡∏¢‡∏µ‡πâ",
            "‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏£‡πà‡∏≠‡∏°", "‡∏Å‡∏£‡∏∞‡πÅ‡∏ó‡∏Å", "‡πÄ‡∏≠‡∏≤‡∏Å‡∏±‡∏ô", "‡πÄ‡∏≠‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ß‡πà‡∏≤", "‡πÄ‡∏≠‡∏≤‡πÅ‡∏£‡∏á‡πÜ", "‡πÄ‡∏≠‡∏≤‡πÅ‡∏£‡∏á‡πÅ‡∏£‡∏á",
            "‡πÄ‡∏•‡∏µ‡∏¢", "‡∏î‡∏π‡∏î",  "‡πÅ‡∏´‡∏¢‡πà", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤", "‡∏™‡∏≠‡∏î‡πÄ‡∏Ç‡πâ‡∏≤",
            "‡∏ï‡πà‡∏≥‡∏ï‡∏°", "‡∏≠‡∏±‡∏õ‡∏£‡∏µ‡∏¢‡πå", "‡∏à‡∏±‡∏ç‡πÑ‡∏£", "‡∏Å‡∏£‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏Å‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏≠‡∏µ‡∏ï‡∏±‡∏ß", "‡∏≠‡∏µ‡πÅ‡∏û‡∏®‡∏¢‡∏≤", "‡πÅ‡∏û‡∏®‡∏¢‡∏≤",
            "‡∏≠‡∏µ‡∏î‡∏≠‡∏Å", "‡∏î‡∏≠‡∏Å‡∏ó‡∏≠‡∏á", "‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ"
        ]

        # 2) Additional variant regex fragments that aren't simple literals
        thai_variant_patterns = [
            r"‡∏Ñ\s*‡∏ß‡∏¢(?:‡∏¢+)?",              # allow spacing and elongation
            r"‡πÄ‡∏≠‡∏≤(?:\s*‡∏Å‡∏±‡∏ô)?",             # with/without ‡∏Å‡∏±‡∏ô, optional spaces
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡πÄ‡∏´‡∏µ‡πâ‡∏¢(?:‡∏¢+)?",   # with/without prefix, elongation
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡∏™‡∏±‡∏î",          
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡∏™‡∏±‡∏™",          
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡πÄ‡∏ß‡∏£(?:‡∏ï‡∏∞‡πÑ‡∏•)?", 
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡∏Ñ‡∏ß‡∏≤‡∏¢",        
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡∏£‡∏∞‡∏¢‡∏≥",        
            r"(?:‡πÑ‡∏≠‡πâ|‡∏≠‡∏µ)?\s*‡∏™‡∏ñ‡∏∏‡∏ô",        
            r"‡∏ä‡∏≤‡∏ï‡∏¥\s*‡∏´‡∏°‡∏≤",                 
            r"‡∏´‡∏ô‡πâ‡∏≤\s*‡∏Ñ‡∏ß‡∏¢",                 
            r"‡∏´‡∏ô‡πâ‡∏≤\s*‡∏´‡∏µ"                   
        ]

        # 3) Build a single Thai regex from the list + variants
        thai_literals_pattern = "|".join(map(re.escape, self.thai_inappropriate_terms))
        # For Thai, use simple pattern matching without strict word boundaries
        # This is more permissive but catches profanity in context
        thai_pattern = rf"({thai_literals_pattern}|{'|'.join(thai_variant_patterns)})"

        # 4) English inappropriate terms (literals) and variant fragments
        self.english_inappropriate_terms = [
            "fuck", "motherfucker", "mf", "shit", "bullshit", "bs", "bitch",
            "slut", "whore", "cunt", "pussy", "dick", "cock", "asshole",
            "ass", "bastard", "son of a bitch", "sob", "bloody hell",
            "goddamn", "god damn", "damn", "prick", "wanker", "twat"
        ]
        english_variant_patterns = [
            r"f\*+k", r"fxxk", r"sh\*t", r"b!tch"
        ]
        english_literals_pattern = "|".join(map(re.escape, self.english_inappropriate_terms))
        english_pattern = rf"\\b({english_literals_pattern}|{'|'.join(english_variant_patterns)})\\b"

        # 5) Security-related generic terms to avoid (non-PDPA-specific)
        self.security_avoid_terms = [
            "crack", "cracker", "cracking", "exploit", "vulnerability", "intrusion",
            "malware", "virus", "trojan", "ransomware", "phishing", "spam", "ddos",
            "sql injection", "xss", "csrf", "buffer overflow", "privilege escalation",
            "rootkit", "backdoor", "keylogger", "spyware", "adware", "botnet", "worm",
            "logic bomb", "time bomb", "easter egg", "trapdoor", "trap door", "trap-door",
            "trap_door"
        ]
        security_pattern = rf"\\b({'|'.join(map(re.escape, self.security_avoid_terms))})\\b"

        # 6) Violence-related terms (Thai/English)
        self.violence_terms_th = ["‡∏Ü‡πà‡∏≤", "‡∏Ü‡∏≤‡∏ï‡∏Å‡∏£‡∏£‡∏°", "‡∏Ü‡∏≤‡∏ï‡∏Å‡∏£"]
        # Use proper word boundaries for Thai
        violence_th_pattern = rf"(?<![‡∏Å-‡πô])({'|'.join(map(re.escape, self.violence_terms_th))})(?![‡∏Å-‡πô])"
        self.violence_terms_en = [
            "kill", "murder", "murderer", "assassinate", "assassin", "execute",
            "execution", "suicide", "suicidal", "terrorism", "terrorist", "bomb",
            "explosion", "gun", "weapon", "violence", "violent", "attack", "assault",
            "threat", "threatening"
        ]
        violence_en_pattern = rf"\\b({'|'.join(map(re.escape, self.violence_terms_en))})\\b"

        # 7) Drug-related terms (Thai/English)
        self.drug_terms_th = ["‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î", "‡∏¢‡∏≤‡πÄ‡∏™‡∏û"]
        # Use proper word boundaries for Thai
        drug_th_pattern = rf"(?<![‡∏Å-‡πô])({'|'.join(map(re.escape, self.drug_terms_th))})(?![‡∏Å-‡πô])"
        self.drug_terms_en = [
            "drug", "drugs", "heroin", "cocaine", "marijuana", "weed", "meth",
            "amphetamine", "ecstasy", "lsd", "pills", "overdose", "addiction",
            "addict", "dealer", "trafficking", "smuggling"
        ]
        drug_en_pattern = rf"\\b({'|'.join(map(re.escape, self.drug_terms_en))})\\b"

        self.inappropriate_patterns = [
            thai_pattern,
            english_pattern,
            security_pattern,
            violence_th_pattern,
            violence_en_pattern,
            drug_th_pattern,
            drug_en_pattern,
        ]
        
        # PDPA keyword heuristic disabled: rely on AI check only
        self.pdpa_keywords = []
        
        # Compile regex patterns for efficiency
        self.inappropriate_regex = re.compile('|'.join(self.inappropriate_patterns), re.IGNORECASE)
        # Prompt-injection heuristics (Thai/English)
        self.injection_phrases = [
            r"ignore (all|any) (previous|prior) (instructions|messages)",
            r"disregard (the )?(rules|system|guardrails)",
            r"act as (?:an?|the) (?:admin|developer|system)",
            r"reveal (?:your )?(?:system|hidden) prompt",
            r"jailbreak|do-anything-now|DAN",
            r"override safety|bypass safety|disable safety",
            r"‡πÄ‡∏û‡∏¥‡∏Å‡πÄ‡∏â‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤|‡∏•‡∏∞‡πÄ‡∏•‡∏¢‡∏Å‡∏é|‡πÅ‡∏™‡∏î‡∏á system prompt|‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢|‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î",
        ]
        self.injection_regex = re.compile('|'.join(self.injection_phrases), re.IGNORECASE)

        # Basic PII patterns (best-effort)
        self.email_regex = re.compile(r"[a-zA-Z0-9_.+\-]+@[a-zA-Z0-9\-]+\.[a-zA-Z0-9\-.]+")
        self.phone_regex = re.compile(r"(?:\+?66|0)[\s\-]?(?:\d[\s\-]?){8,10}")
        # Thai National ID (13 digits with hyphens optional)
        self.thai_id_regex = re.compile(r"\b\d{1}-?\d{4}-?\d{5}-?\d{2}-?\d{1}\b")
        
        # Logging setup
        self.logger = logging.getLogger(__name__)
    
    def check_content_safety(self, text: str) -> Tuple[bool, List[str]]:
        """
        Content safety check using Guardrails profanity validator if available,
        with fallback to regex-based detection of severe profanity/violence, etc.
        
        Args:
            text: The text to check
            
        Returns:
            Tuple of (is_safe, list_of_violations)
        """
        if not text:
            return True, []
        
        violations = []
        text_lower = text.lower()

        # 0) Prefer Guardrails' ProfanityFree if available
        if self._profanity_validator is not None:
            try:
                # Guardrails validators typically expose a __call__/validate-like interface; support both
                is_clean = True
                message = None
                if hasattr(self._profanity_validator, "validate"):
                    result = self._profanity_validator.validate(text_lower)  # type: ignore
                    # Try to interpret common result shapes
                    if isinstance(result, tuple) and len(result) >= 2:
                        # (validated_value, error)
                        _, err = result[0], result[1]
                        is_clean = err is None
                        message = None if err is None else str(err)
                    elif isinstance(result, dict):
                        # {is_valid: bool, error: str}
                        is_clean = bool(result.get("is_valid", True))
                        message = result.get("error")
                    else:
                        # If no error thrown, assume valid
                        is_clean = True
                else:
                    # Attempt callable
                    _ = self._profanity_validator(text_lower)  # type: ignore
                    is_clean = True

                if not is_clean:
                    violations.append("‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏≥‡∏´‡∏¢‡∏≤‡∏ö‡∏à‡∏≤‡∏Å Guardrails Profanity validator" if not message else message)
            except Exception:
                # If guardrails call fails, continue with regex fallback
                pass
        
        # 1) Regex-based fallback for inappropriate content
        matches = self.inappropriate_regex.findall(text_lower)
        if matches:
            flat_matches = []
            for match in matches:
                if isinstance(match, tuple):
                    flat_matches.extend([m for m in match if m])
                else:
                    flat_matches.append(match)
            if flat_matches:
                violations.append(f"‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°: {', '.join(set(flat_matches))}")
        
        # Only block extreme security terms that are clearly malicious
        extreme_security_terms = [
            "create malware", "create virus", "create trojan", "create ransomware",
            "make malware", "make virus", "make trojan", "make ransomware",
            "build malware", "build virus", "build trojan", "build ransomware",
            "develop malware", "develop virus", "develop trojan", "develop ransomware"
        ]
        
        found_extreme_terms = []
        for term in extreme_security_terms:
            if term.lower() in text_lower:
                found_extreme_terms.append(term)
        
        if found_extreme_terms:
            violations.append(f"‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏±‡∏•‡πÅ‡∏ß‡∏£‡πå: {', '.join(found_extreme_terms)}")
        
        is_safe = len(violations) == 0
        
        if not is_safe:
            self.logger.warning(f"Content safety violation detected: {violations}")
        
        return is_safe, violations

    def _ai_check_pdpa_related(self, text: str) -> Tuple[bool, str]:
        """
        Determine PDPA-relatedness using ONLY the AI judgment (per request).
        If the AI is not available, treat as not related and ask user to ask about PDPA.
        Returns (is_related, reason_text).
        """
        if not text or self._openai_client is None:
            return False, "AI unavailable for PDPA check"

        try:
            system = (
                "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡∏°‡∏≤ "
                "‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA (‡∏û.‡∏£.‡∏ö. ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà "
                "‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á' ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏° ‡πÄ‡∏ä‡πà‡∏ô "
                "‡∏Å‡∏£‡∏ì‡∏µ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏á‡πÄ‡∏≠‡∏¥‡∏ç‡∏ï‡∏¥‡∏î‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏≥‡πÑ‡∏õ‡πÇ‡∏û‡∏™‡∏ï‡πå ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á "
                "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏¥‡∏ô‡∏¢‡∏≠‡∏°. "
                "‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö '‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á' ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÜ"
            )
            prompt = f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {text}\n‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
            resp = self._openai_client.chat.completions.create(
                model=self.llama_model,
                messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt}],
                max_tokens=64,
                temperature=0
            )
            content = (resp.choices[0].message.content or "").strip()
            norm = content.replace("\u200b", "").strip().lower()
            # Accept either original Y:/N: format or Thai keywords
            if norm.startswith("y:") or norm.startswith("‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"):
                return True, content
            if norm.startswith("n:") or norm.startswith("‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"):
                return False, content
            # If AI didn't follow format, default to not related
            return False, content or "‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI"
        except Exception as e:
            return False, "AI error during PDPA check"

    def _is_severe_profanity(self, text: str, violations: List[str]) -> bool:
        """
        Check if the profanity is severe enough to block the question.
        
        Args:
            text: The text to check
            violations: List of safety violations found
            
        Returns:
            True if the profanity is severe, False otherwise
        """
        if not violations:
            return False
        
        # Define severe profanity patterns that should always be blocked
        severe_patterns = [
            # Sexual content
            "‡πÄ‡∏¢‡πá‡∏î", "‡πÄ‡∏≠‡∏≤", "‡∏õ‡∏µ‡πâ", "‡∏™‡∏≠‡∏î", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ö", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ß", "‡πÄ‡∏á‡∏µ‡πà‡∏¢‡∏ô", "‡∏Ç‡πà‡∏°‡∏Ç‡∏∑‡∏ô", "‡∏£‡∏∏‡∏°‡πÇ‡∏ó‡∏£‡∏°",
            "‡∏ô‡πâ‡∏≥‡πÅ‡∏ï‡∏Å", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏ô", "‡∏™‡∏ß‡∏¥‡∏á‡∏Å‡∏¥‡πâ‡∏á", "‡∏™‡∏ß‡∏¥‡πâ‡∏á‡∏Å‡∏¥‡πâ‡∏á", "‡∏î‡∏π‡∏î‡∏õ‡∏≤‡∏Å", "‡∏≠‡∏°‡∏Ñ‡∏ß‡∏¢", "‡πÄ‡∏•‡∏µ‡∏¢‡∏´‡∏µ",
            "‡∏Ñ‡∏ß‡∏¢‡πÉ‡∏´‡∏ç‡πà", "‡∏Ñ‡∏ß‡∏¢‡∏¢‡∏≤‡∏ß", "‡∏Ñ‡∏ß‡∏¢‡πÄ‡∏î‡πá‡∏Å", "‡∏´‡∏µ‡∏ö‡∏≤‡∏ô", "‡∏´‡∏µ‡πÄ‡∏î‡πá‡∏Å", "‡∏´‡∏µ‡πÄ‡∏ô‡πà‡∏≤", "‡∏à‡∏¥‡πã‡∏°‡πÄ‡∏î‡πá‡∏Å",
            "‡πÅ‡∏ï‡∏Å‡∏Ñ‡∏≤‡∏õ‡∏≤‡∏Å", "‡πÅ‡∏ï‡∏Å‡∏Ñ‡∏≤", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏™‡πà‡∏´‡∏ô‡πâ‡∏≤", "‡πÅ‡∏ï‡∏Å‡πÉ‡∏™‡πà‡∏õ‡∏≤‡∏Å", "‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏ô", "‡∏Ç‡∏¢‡πà‡∏°", "‡∏Ç‡∏¢‡∏µ‡πâ",
            "‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏£‡πà‡∏≠‡∏°", "‡∏Å‡∏£‡∏∞‡πÅ‡∏ó‡∏Å", "‡πÄ‡∏≠‡∏≤‡∏Å‡∏±‡∏ô", "‡πÄ‡∏≠‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ß‡πà‡∏≤", "‡πÄ‡∏≠‡∏≤‡πÅ‡∏£‡∏á‡πÜ", "‡πÄ‡∏≠‡∏≤‡πÅ‡∏£‡∏á‡πÅ‡∏£‡∏á",
            "‡πÄ‡∏•‡∏µ‡∏¢", "‡∏î‡∏π‡∏î", "‡∏≠‡∏°", "‡∏¢‡∏±‡∏î", "‡πÅ‡∏´‡∏¢‡πà", "‡πÄ‡∏™‡∏µ‡∏¢‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤", "‡∏™‡∏≠‡∏î‡πÄ‡∏Ç‡πâ‡∏≤",
            
            # Extreme insults
            "‡∏û‡πà‡∏≠‡∏°‡∏∂‡∏á‡∏ï‡∏≤‡∏¢", "‡πÅ‡∏°‡πà‡∏°‡∏∂‡∏á‡∏ï‡∏≤‡∏¢", "‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢", "‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢‡∏ß‡∏≠‡∏î", "‡πÑ‡∏≠‡πâ‡∏™‡∏≤‡∏£‡πÄ‡∏•‡∏ß", "‡∏™‡∏≤‡∏£‡πÄ‡∏•‡∏ß",
            "‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏≤‡∏ô", "‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏ô‡∏≤", "‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏°‡∏µ‡∏¢", "‡∏Å‡∏≤‡∏Å", "‡∏Ç‡∏¢‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°",
            "‡∏Å‡∏£‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏Å‡∏∞‡∏´‡∏£‡∏µ‡πà", "‡∏Å‡∏£‡∏∞‡∏£‡∏µ‡πà", "‡∏Å‡∏∞‡∏£‡∏µ‡πà", "‡∏≠‡∏µ‡∏ï‡∏±‡∏ß", "‡∏≠‡∏µ‡πÅ‡∏û‡∏®‡∏¢‡∏≤", "‡πÅ‡∏û‡∏®‡∏¢‡∏≤",
            "‡∏≠‡∏µ‡∏î‡∏≠‡∏Å", "‡∏î‡∏≠‡∏Å‡∏ó‡∏≠‡∏á", "‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ß‡∏¢", "‡πÑ‡∏≠‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏µ",
            
            # English severe profanity
            "fuck", "motherfucker", "cunt", "pussy", "dick", "cock", "asshole",
            "bitch", "slut", "whore", "bastard", "son of a bitch"
        ]
        
        text_lower = text.lower()
        
        # Check for severe patterns
        for pattern in severe_patterns:
            if pattern.lower() in text_lower:
                return True
        
        # Check if multiple mild profanities are used together
        mild_profanities = ["‡∏°‡∏∂‡∏á", "‡∏Å‡∏π", "‡πÄ‡∏´‡∏µ‡πâ‡∏¢", "‡πÄ‡∏Æ‡∏µ‡πâ‡∏¢", "‡πÄ‡∏´‡πâ", "‡∏´‡πà‡∏≤", "‡∏™‡∏±‡∏î", "‡∏™‡∏±‡∏™", "‡πÄ‡∏ä‡∏µ‡πà‡∏¢", "‡πÄ‡∏ä‡∏µ‡πâ‡∏¢", "‡πÅ‡∏°‡πà‡∏á"]
        mild_count = sum(1 for word in mild_profanities if word in text_lower)
        
        # If more than 2 mild profanities, consider it severe
        if mild_count > 2:
            return True
        
        return False

    def detect_prompt_injection(self, text: str) -> List[str]:
        """
        Detect likely prompt-injection attempts using heuristic patterns.
        Returns list of matched phrases (empty if none).
        """
        if not text:
            return []
        matches = self.injection_regex.findall(text)
        # Normalize matches to strings
        flat = []
        for m in matches:
            if isinstance(m, tuple):
                flat.extend([x for x in m if x])
            else:
                flat.append(m)
        return list(set(flat))

    def sanitize_pii(self, text: str) -> str:
        """
        Redact common PII patterns from text.
        """
        if not text:
            return text
        redacted = self.email_regex.sub('[REDACTED_EMAIL]', text)
        redacted = self.phone_regex.sub('[REDACTED_PHONE]', redacted)
        redacted = self.thai_id_regex.sub('[REDACTED_THAI_ID]', redacted)
        return redacted
    
    def check_topic_restriction(self, text: str) -> Tuple[bool, List[str]]:
        """
        Check if the text is related to PDPA topics only.
        
        Args:
            text: The text to check
            
        Returns:
            Tuple of (is_pdpa_related, list_of_reasons)
        """
        if not text:
            return False, ["‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤"]
        
        text_lower = text.lower()
        
        # Check if text contains PDPA-related keywords
        pdpa_matches = []
        for keyword in self.pdpa_keywords:
            if keyword.lower() in text_lower:
                pdpa_matches.append(keyword)
        
        if pdpa_matches:
            return True, [f"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA: {', '.join(pdpa_matches[:3])}"]
        
        # Check for general legal/privacy terms that might be PDPA-related
        legal_privacy_terms = [
            "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
            "privacy", "legal", "law", "regulation", "compliance", "governance",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
            "data", "personal", "private", "confidential", "sensitive",
            # Add more general terms that could be related to privacy/data protection
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢", "security", "protection", "‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á", "protect",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß", "privacy", "‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß", "private",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "information", "data", "‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®",
            "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "law", "regulation", "‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö",
            "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£", "management", "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "administration"
        ]
        
        legal_matches = []
        for term in legal_privacy_terms:
            if term.lower() in text_lower:
                legal_matches.append(term)
        
        if legal_matches:
            return True, [f"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢/‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß: {', '.join(legal_matches[:3])}"]
        
        return False, ["‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"]
    
    def filter_user_input(self, user_input: str) -> Dict[str, any]:
        """
        [CORRECTED LOGIC]
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
        1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ (Injection) -> ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        2. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (Profanity) -> ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PDPA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà -> ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß
        """
        # --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ---
        result = {
            "is_safe": True,
            "is_pdpa_related": True,
            "violations": [],
            "reasons": [],
            "filtered_text": user_input,
            "should_respond": True,
            "response_message": ""
        }

        # --- ‡∏î‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ Prompt Injection (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ---
        injection_hits = self.detect_prompt_injection(user_input)
        if injection_hits:
            result["should_respond"] = False
            result["response_message"] = "‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏£‡∏∞‡∏ö‡∏ö"
            result["violations"].append(f"Prompt-injection attempt: {', '.join(injection_hits)}")
            return result

        # --- ‡∏î‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° / ‡∏Ñ‡∏≥‡∏´‡∏¢‡∏≤‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ---
        is_safe, safety_violations = self.check_content_safety(user_input)
        if not is_safe:
            # ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏´‡∏¢‡∏≤‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            result["should_respond"] = False
            result["response_message"] = "üî¥ [Guardrail] ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏ö‡∏Ñ‡∏≥‡∏´‡∏¢‡∏≤‡∏ö/‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
            result["violations"].extend(safety_violations)
            return result

        # --- ‡∏î‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÉ‡∏ä‡πâ AI ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ---
        is_pdpa_related, reason_text = self._ai_check_pdpa_related(user_input)
        if not is_pdpa_related:
            result["should_respond"] = False
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PDPA ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ô‡∏∞‡πÅ‡∏ô‡∏ß
            result["response_message"] = (
                "üî¥ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA\n\n"
                f"‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {reason_text}"
            )
            if reason_text:
                result["reasons"].append(reason_text)
            return result

        # --- ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏î‡πà‡∏≤‡∏ô ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ Input ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ---
        return result
    
    def filter_ai_response(self, ai_response: str) -> Dict[str, any]:
        """
        Filter AI-generated responses for safety.
        
        Args:
            ai_response: The AI's response text
            
        Returns:
            Dictionary with filtering results
        """
        result = {
            "is_safe": True,
            "violations": [],
            "filtered_text": ai_response,
            "should_display": True,
            "replacement_message": ""
        }
        
        # Redact PII first
        ai_response = self.sanitize_pii(ai_response)

        # Check content safety
        is_safe, safety_violations = self.check_content_safety(ai_response)
        result["is_safe"] = is_safe
        result["violations"].extend(safety_violations)
        
        if not is_safe:
            result["should_display"] = False
            result["replacement_message"] = (
                "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ "
                "‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDPA"
            )
        
        # Detect prompt injection language in the response (unlikely, but safe)
        injection_hits = self.detect_prompt_injection(ai_response)
        if injection_hits:
            result["violations"].append(
                f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô prompt-injection: {', '.join(injection_hits[:3])}"
            )
        return result
    
    def sanitize_text(self, text: str) -> str:
        """
        Sanitize text by removing or replacing inappropriate content.
        """
        if not text:
            return text

        sanitized = self.inappropriate_regex.sub('[REDACTED]', text)
        sanitized = self.sanitize_pii(sanitized)
        return sanitized
